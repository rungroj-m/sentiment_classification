{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rungrojmaipradit/anaconda3/lib/python3.7/site-packages/pyparsing.py:2725: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile( self.reString )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow\n",
      "total_n_gram 3050\n",
      "top_vector 3050\n",
      "finish\n",
      "{'type': 0, 'comment': 'Gotcha ', 'vector': {}} {}\n",
      "{'type': 0, 'comment': 'Trust URI ', 'vector': {}} {}\n",
      "{'type': 0, 'comment': 'InsertandRetriveBlobData ', 'vector': {}} {}\n",
      "{'type': 0, 'comment': 'Hmmm ', 'vector': {}} {}\n",
      "{'type': 1, 'comment': 'Worked ', 'vector': {}} {}\n",
      "{'type': 1, 'comment': 'Congratulations ', 'vector': {}} {}\n",
      "coverage ratio: 99.6\n",
      "OrderedDict([(0, 6), (1, 16), (2, 33), (3, 45), (4, 45), (5, 62), (6, 53), (7, 69), (8, 72), (9, 70), (10, 72), (11, 76), (12, 79), (13, 82), (14, 62), (15, 63), (16, 42), (17, 70), (18, 44), (19, 32), (20, 37), (21, 44), (22, 34), (23, 39), (24, 35), (25, 33), (26, 23), (27, 16), (28, 18), (29, 19), (30, 11), (31, 15), (32, 9), (33, 8), (34, 11), (35, 9), (36, 5), (37, 4), (38, 6), (39, 3), (40, 4), (41, 4), (42, 2), (44, 3), (45, 1), (46, 2), (47, 1), (49, 4), (50, 1), (52, 1), (53, 2), (55, 2), (59, 1)])\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from myconfig import *\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "N_GRAM_LENGTH = 10\n",
    "TOTAL_TYPE = 2\n",
    "summary = dict()\n",
    "total_n_gram = 0\n",
    "total_document = 0\n",
    "score = dict()\n",
    "id_n_gram_mapping = dict()\n",
    "comment_summary = dict()\n",
    "top_vector = list()\n",
    "n_grams = dict()\n",
    "## Change project here\n",
    "project_name = \"appreviews\"\n",
    "print(\"stratified shuffle split \"+project_name)\n",
    "\n",
    "# negative = -1, neutral = 0, postitive = 1\n",
    "# weight = log(|D|/sdf) * gtf\n",
    "\n",
    "def read_raw_data():\n",
    "    with open(DATA_SOURCE+\"/dataset.csv\", 'r',encoding='latin1') as csvfile:\n",
    "        global total_document\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['project'] == project_name:\n",
    "                comment_summary[total_document] = dict()\n",
    "                comment_summary[total_document]['type'] = int(row['oracle'])\n",
    "                comment_summary[total_document]['comment'] = row['text']\n",
    "                total_document += 1\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1   \n",
    "    \n",
    "#read data from n_gram\n",
    "def read_n_gram():\n",
    "    n_gram_id = 0\n",
    "    total_n_gram = file_len(DATA_SOURCE+\"/\"+project_name+\"_n_gram_filter\")\n",
    "    print(\"total_n_gram\",total_n_gram)\n",
    "    with open(DATA_SOURCE+\"/\"+project_name+\"_n_gram_filter\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        for row in reader:\n",
    "            words = row[5].strip()\n",
    "            term = tuple(row[5].strip().split(' '))\n",
    "            if term not in summary:\n",
    "                summary[term] = dict()\n",
    "            summary[term] = {'id':n_gram_id,'len':row[1],'gtf':row[2],'df':row[3],'sdf':row[4], 'term':row[5]}\n",
    "            weight1 = math.log10(total_document / int(row[4]))\n",
    "            summary[term]['score'] = weight1 * int(row[2])\n",
    "            score[n_gram_id] =  weight1 * int(row[2])\n",
    "#            weight2 = math.log10(total_document * int(row[3]) / (int(row[4]) * int(row[4])) )\n",
    "#            summary[term]['score'] = weight2\n",
    "#            score[n_gram_id] =  weight2\n",
    "            n_grams[n_gram_id] = term\n",
    "            n_gram_id += 1\n",
    "\n",
    "def top_score_vector():\n",
    "    percent = int(len(score) * 100 / 100)\n",
    "    top_vector.extend(sorted(score,key=score.get,reverse=True)[:percent])\n",
    "#     top_vector.extend(list(filter(lambda w: score[w] > 1, score)))\n",
    "    print(\"top_vector\",len(top_vector))\n",
    "\n",
    "def n_gram_split():\n",
    "    for comment_index in comment_summary:\n",
    "        comment = comment_summary[comment_index]['comment']\n",
    "        comment_summary[comment_index]['vector'] = dict()\n",
    "        comment_post_process = re.sub(\"\\s+\",\" \",re.sub(r\"[^A-Za-z0-9]+\",\" \",comment.replace(\"\\t\",\" \").replace(\"\\r\\n\",\" \").lower())).split(\" \")\n",
    "        for i in range(len(comment_post_process)):\n",
    "            for j in range(i,min(i+N_GRAM_LENGTH+1,len(comment_post_process))):\n",
    "                if(tuple(comment_post_process[i:j+1]) in summary):\n",
    "                    if(summary[tuple(comment_post_process[i:j+1])]['id'] in top_vector):\n",
    "                        if summary[tuple(comment_post_process[i:j+1])]['id'] in comment_summary[comment_index]['vector']:\n",
    "                            comment_summary[comment_index]['vector'][summary[tuple(comment_post_process[i:j+1])]['id']] += 1\n",
    "                        else: \n",
    "                            comment_summary[comment_index]['vector'][summary[tuple(comment_post_process[i:j+1])]['id']] = 1\n",
    "                            \n",
    "def vector_idf():\n",
    "    for i in comment_summary:\n",
    "         for v in comment_summary[i]['vector']:\n",
    "            comment_summary[i]['vector'][v] *= score[v]\n",
    "\n",
    "def cal_coverage():\n",
    "    empty_vector = 0\n",
    "    total_vector = len(comment_summary)\n",
    "    for i in comment_summary:\n",
    "        if not comment_summary[i]['vector']:\n",
    "            empty_vector += 1\n",
    "            print(comment_summary[i],comment_summary[i]['vector'])\n",
    "    print(\"coverage ratio:\",(total_vector - empty_vector)/total_vector * 100)\n",
    "\n",
    "def cal_coverage_freq():\n",
    "    empty_vector = 0\n",
    "    total_vector = len(comment_summary)\n",
    "    n_gram_per_comment = defaultdict(lambda:0)\n",
    "    for i in comment_summary:\n",
    "        sum = 0\n",
    "        for j in comment_summary[i]['vector']:\n",
    "            sum += 1\n",
    "        n_gram_per_comment[sum] += 1\n",
    "    print(OrderedDict(sorted(n_gram_per_comment.items(), key=lambda t: t[0])))\n",
    "    \n",
    "read_raw_data()\n",
    "read_n_gram()\n",
    "top_score_vector()\n",
    "n_gram_split()\n",
    "print(\"finish\")\n",
    "#vector_idf()\n",
    "cal_coverage()\n",
    "cal_coverage_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# currently we have top_score_vector and comment&vector\n",
    "# create x and y -> x contains vector and y contains result\n",
    "X = []\n",
    "y = []\n",
    "print(\"test\")\n",
    "for comment_index in comment_summary:\n",
    "    vector = [0] * len(top_vector)\n",
    "    for vector_index in comment_summary[comment_index]['vector']:\n",
    "        vector[top_vector.index(vector_index)] = comment_summary[comment_index]['vector'][vector_index]\n",
    "    X.append(vector)\n",
    "    y.append(comment_summary[comment_index]['type'])\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2019-01-15 14:23:50,015:EnsembleBuilder(1):549cc566db55aa91e299a80b32babd66] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-01-15 14:23:50,030:EnsembleBuilder(1):549cc566db55aa91e299a80b32babd66] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-01-15 14:23:52,036:EnsembleBuilder(1):549cc566db55aa91e299a80b32babd66] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-01-15 14:24:02,063:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2019-01-15 14:24:02,063:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:56] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "[14:25:57] src/gbm/gbtree.cc:492: drop 0 trees, weight = 1\n",
      "round: 0 Classification report              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.70      0.39      0.50        18\n",
      "          0       0.85      0.97      0.90       119\n",
      "          1       0.75      0.23      0.35        13\n",
      "\n",
      "avg / total       0.82      0.83      0.81       150\n",
      "\n",
      "round: 0 Confusion matrix [[  7  11   0]\n",
      " [  3 115   1]\n",
      " [  0  10   3]]\n",
      "round: 0 AUC 0.6398090960134756\n",
      "number of process 70\n",
      "status ['Success', 'Crash', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Timeout', 'Success', 'Success', 'Success', 'Crash', 'Success', 'Success', 'Success', 'Timeout', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Timeout', 'Crash', 'Success', 'Crash', 'Crash', 'Success', 'Success', 'Crash', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Success', 'Timeout']\n",
      "mean_test_score [0.78499313 0.         0.64806643 0.70829255 0.74429673 0.72922296\n",
      " 0.77874306 0.70793279 0.74654311 0.70244395 0.62430589 0.66682885\n",
      " 0.61651418 0.47528889 0.         0.73958117 0.70244395 0.70244395\n",
      " 0.         0.73817441 0.77773978 0.70244395 0.         0.70244395\n",
      " 0.01406315 0.73072437 0.7800239  0.68678235 0.70682139 0.75487567\n",
      " 0.70244395 0.70244395 0.70244395 0.         0.         0.72601732\n",
      " 0.         0.         0.70244395 0.79085258 0.         0.70244395\n",
      " 0.7360867  0.76322498 0.70463649 0.72702949 0.70244395 0.74909069\n",
      " 0.76801385 0.68831945 0.71051877 0.73837629 0.70244395 0.71149607\n",
      " 0.70244395 0.73957981 0.70244395 0.73999302 0.7179955  0.64134643\n",
      " 0.70244395 0.53796171 0.70244395 0.73240175 0.75955539 0.70244395\n",
      " 0.74326139 0.74750578 0.73404096 0.        ]\n",
      "{'balancing:strategy': 'weighting', 'categorical_encoding:__choice__': 'no_encoding', 'classifier:__choice__': 'passive_aggressive', 'imputation:strategy': 'median', 'preprocessor:__choice__': 'liblinear_svc_preprocessor', 'rescaling:__choice__': 'none', 'classifier:passive_aggressive:C': 0.009242758088864442, 'classifier:passive_aggressive:average': 'True', 'classifier:passive_aggressive:fit_intercept': 'True', 'classifier:passive_aggressive:loss': 'hinge', 'classifier:passive_aggressive:tol': 0.00030606938603723317, 'preprocessor:liblinear_svc_preprocessor:C': 0.18772543186894908, 'preprocessor:liblinear_svc_preprocessor:dual': 'False', 'preprocessor:liblinear_svc_preprocessor:fit_intercept': 'True', 'preprocessor:liblinear_svc_preprocessor:intercept_scaling': 1, 'preprocessor:liblinear_svc_preprocessor:loss': 'squared_hinge', 'preprocessor:liblinear_svc_preprocessor:multi_class': 'ovr', 'preprocessor:liblinear_svc_preprocessor:penalty': 'l1', 'preprocessor:liblinear_svc_preprocessor:tol': 6.797356625538871e-05}\n",
      "round: 0 comment index: 1387 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 140 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 1424 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 1459 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 1430 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 1472 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 146 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 79 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 1433 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 25 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 137 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 884 y_test: 0 y_hat -1\n",
      "round: 0 comment index: 1373 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 407 y_test: 0 y_hat 1\n",
      "round: 0 comment index: 129 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 1376 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 264 y_test: 0 y_hat -1\n",
      "round: 0 comment index: 157 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 1461 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 629 y_test: 0 y_hat -1\n",
      "round: 0 comment index: 37 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 139 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 1371 y_test: 1 y_hat 0\n",
      "round: 0 comment index: 133 y_test: -1 y_hat 0\n",
      "round: 0 comment index: 72 y_test: -1 y_hat 0\n",
      "AUC [0.6398090960134756] 0.6398090960134756\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10,random_state=1)\n",
    "np_X,np_y = np.asarray(X),np.asarray(y)\n",
    "sss.get_n_splits(np_X, np_y)\n",
    "runner = 0\n",
    "\n",
    "measure_scores = dict()\n",
    "# measure_scores['precision'] = list()\n",
    "# measure_scores['recall'] = list()\n",
    "# measure_scores['f1'] = list()\n",
    "measure_scores['AUC'] = list()\n",
    "for train_index, test_index in sss.split(np_X,np_y):\n",
    "    X_train, X_test = np_X[train_index], np_X[test_index]\n",
    "    y_train, y_test = np_y[train_index], np_y[test_index]\n",
    "    automl = autosklearn.classification.AutoSklearnClassifier(ml_memory_limit=1024*64,time_left_for_this_task=90*60,per_run_time_limit=15*60)\n",
    "    automl.fit(X_train.copy(), y_train.copy(),metric=autosklearn.metrics.f1_weighted)\n",
    "    automl.refit(X_train.copy(), y_train.copy())\n",
    "    y_hat = automl.predict(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_hat, pos_label=1)\n",
    "    measure_scores['AUC'].append(auc(fpr, tpr))\n",
    "    log = automl.cv_results_\n",
    "    with open(project_name+\"_sss_\"+str(runner)+\".txt\",\"w\") as file:\n",
    "        file.write(\"log for \"+str(runner)+\"\\n\")\n",
    "        file.write(str(log)+\"\\n\")\n",
    "    \n",
    "    print(\"round:\",runner,\"Classification report\", classification_report(y_test, y_hat))\n",
    "    print(\"round:\",runner,\"Confusion matrix\", confusion_matrix(y_test, y_hat))\n",
    "    print(\"round:\",runner,\"AUC\",auc(fpr,tpr))\n",
    "    print(\"number of process\",len(log['rank_test_scores']))\n",
    "    print(\"status\",log['status'])\n",
    "    print(\"mean_test_score\",log['mean_test_score'])\n",
    "    \n",
    "    for i in np.where(log['rank_test_scores']==1)[0] :\n",
    "        print(log['params'][i])     \n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_hat[i]:\n",
    "            print(\"round:\",runner,\"comment index:\",test_index[i],\"y_test:\",y_test[i],\"y_hat\",y_hat[i])\n",
    "        \n",
    "    runner += 1\n",
    "\n",
    "print('AUC',measure_scores['AUC'],np.mean(measure_scores['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
